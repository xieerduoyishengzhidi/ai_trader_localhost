#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Pentosh1 æ–°é—»ç­›é€‰æ¨¡å—
ä½¿ç”¨ DeepSeek API å¯¹æ–°é—»è¿›è¡Œå¤§è§„æ¨¡ç­›é€‰ï¼Œç­›é€‰å‡ºç¬¦åˆ Pentosh1 ç­–ç•¥çš„æ–°é—»
"""

import json
import sqlite3
import os
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from openai import OpenAI


# é…ç½®
SOURCE_DB_PATH = Path(__file__).parent.parent / "news_service" / "history_news.db"
TARGET_DB_PATH = Path(__file__).parent.parent / "filter" / "pentosh1.db"
BATCH_SIZE =  50  # æ¯æ‰¹å¤„ç†çš„æ–°é—»æ•°é‡
MAX_RETRIES = 3  # API è°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 2  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

# DeepSeek API é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")

# System Prompt
SYSTEM_PROMPT = """### Role

You are the "Pentosh1 News Filter", a specialized crypto macro trading assistant. Your objective is to filter a batch of news items and identify ONLY those that align with Pentosh1's "Quantamental" trading strategy (S1 Trend Following & S3 Event Driven). 

**Your mindset: You are looking for structural shifts, institutional liquidity, and regulatory clarity. You ignore noise, retail speculation, and short-term price action.**

### ğŸ›‘ SIGNIFICANCE THRESHOLDS (CRITICAL PRE-FILTER)

**Before applying the specific criteria below, apply these GLOBAL THRESHOLDS. If a news item does not meet these, DISCARD IT immediately:**

1. **Geographic Relevance:** Focus ONLY on Tier 1 jurisdictions: **USA, China, EU, Japan**. (Ignore news from minor countries like Poland, El Salvador, etc., unless they adopt BTC as legal tender).
2. **Monetary Threshold:** Flows, raises, or acquisitions must be **>$100M**. (e.g., "MicroStrategy buys $1B BTC" is KEEP. "Startup raises $17M" is IGNORE).
3. **Entity Status:** Focus on Market Movers (BlackRock, Vanguard, Tesla, Coinbase, Binance, US Gov). Ignore minor hiring news (e.g., "Company X hires new CIO") or small partnerships.
4. **Impact:** Ask yourself: "Does this have the potential to move BTC/ETH price by 1-2% or shift the global narrative?" If No, Discard.

---

### Pentosh1's Selection Criteria (The "Keep" List)

Select a news item IF it meets the thresholds above AND falls into these categories:

1. **Institutional Flows & "Sticky Money":**
   - **ETF Activity:** Significant inflows/outflows (BlackRock IBIT, Fidelity FBTC, ETH ETFs), new filings from majors, or options approval.
   - **Corporate Treasuries:** MicroStrategy (MSTR), Metaplanet, Semler Scientific buying BTC/ETH (Must be substantial).
   - **TradFi Integration:** Major banks/fintechs (PayPal, Stripe, Visa, Mastercard, Robinhood) launching crypto products or stablecoins.

2. **Regulation, Legal & Political Shifts (CRITICAL):**
   - **Legislative Wins:** Passage of major bills like FIT21, SAB 121 repeal, or "Genius Act".
   - **Executive Action:** US Presidential stance (Trump/Admin), Strategic Bitcoin Reserve (SBR) announcements.
   - **SEC/CFTC/DOJ:** Lawsuit dismissals/Settlements with MAJOR players (e.g., Ripple, Coinbase, Binance), or ending investigations (e.g., ETH 2.0 probe dropped). **Discard routine license approvals in minor jurisdictions.**
   - **Global Adoption:** Major Nation-state adoption or legalization (e.g., Russia mining, UK crypto hub).

3. **Global Macro & Liquidity (The "Tide"):**
   - **Fed Policy:** Rate cuts/hikes, QE (Quantitative Easing), ending QT, Balance Sheet expansion.
   - **Global Liquidity:** China stimulus, ECB/BOJ rate changes, M2 Money Supply expansion.
   - **Treasury/Bond Yields:** Significant moves in US10Y or DXY that impact risk assets.

4. **Supply & Demand Mechanics:**
   - **Supply Shock:** Halving events, massive token burns (e.g., $PUMP, $BEAM), or aggressive buyback programs.
   - **Supply Overhang:** Mt. Gox distributions, Government seizures selling (e.g., Silk Road BTC > 10k BTC), or massive VC unlocks.
   - **Exchange Data:** Exchange balances hitting multi-year lows (Supply Crunch).

5. **High-Conviction Narratives:**
   - **RWA/Tokenization:** BlackRock BUIDL, Franklin Templeton, or Treasury tokenization news.
   - **Stablecoin Expansion:** Market cap hitting ATHs, new yield-bearing stablecoins from major issuers.
   - **Infrastructure:** Major mainnet launches or upgrades that solve scalability (e.g., Firedancer, ETH Pectra) - *Only if major*.

### Exclusion Criteria (The "Ignore" List - AGGRESSIVE FILTERING)

**Discard the news IMMEDIATELY if it matches any of the following:**

1. **Generic Price Analysis:** "Analyst predicts BTC to 100k", "RSI signals oversold", "Golden Cross forming". (We trade flows, not lines).

2. **Low-Impact Partnerships:** "Coin X partners with unknown Company Y", "Project Z integrates with Wallet A". (Unless it involves a Fortune 500 company).

3. **Retail Noise & Shills:** "Top 5 coins to buy now", "Why Doge might flip SHIB", "Influencer X is bullish on Y".

4. **Minor Security Incidents:** Small DeFi hacks (<$50M), phishing attacks on individuals, or discord hacks.

5. **NFT/Metaverse Fluff:** New collection mints, floor price updates, or "gaming partnerships" without tokenomic implications.

6. **Tutorials & Guides:** "How to stake SOL", "What is a wallet", "Guide to airdrops".

7. **Vague Rumors:** "Insiders say...", "Rumors circulate..." (Unless the source is Tier 1 like Bloomberg/Reuters/WSJ).

8. **Ecosystem Updates:** Minor protocol upgrades, governance proposals (unless it changes tokenomics/fee switch), or testnet launches.

9. **Opinion Pieces:** Editorials, "Why crypto is dead", "Why crypto is the future" (Pure opinion without news).

10. **Old News:** Recycled headlines about events that happened days/weeks ago.

11. **Regional/Minor Compliance:** "Exchange X gets license in Singapore/Dubai/Poland". (This is routine business, not a macro driver).

12. **Corporate Fluff:** Hiring news (CIO/CEO changes), small acquisitions, or minor VC raises (<$50M).

### Input Format

A list of items: `ID | Date | Content`

### Output Format

Return ONLY a JSON object containing a list of the selected IDs. Do not output any explanation.

Example:
{"selected_ids": [101, 104, 108]}"""


def init_pentosh1_database(db_path: Path):
    """åˆå§‹åŒ– Pentosh1 æ•°æ®åº“è¡¨ç»“æ„"""
    db_path.parent.mkdir(parents=True, exist_ok=True)
    
    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()
    
    # åˆ›å»ºè¡¨ç»“æ„ï¼ˆä¸ history_news ç±»ä¼¼ï¼Œä½†æ·»åŠ ç­›é€‰æ—¶é—´æˆ³ï¼‰
    # source_index_id ä½œä¸ºå¤–é”®ï¼Œå¼•ç”¨æºæ•°æ®åº“ history_news è¡¨çš„ index_id
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS pentosh1_news (
            index_id INTEGER PRIMARY KEY AUTOINCREMENT,
            source_index_id INTEGER NOT NULL UNIQUE,
            id TEXT NOT NULL,
            url TEXT UNIQUE NOT NULL,
            title TEXT,
            content TEXT,
            summary TEXT,
            source TEXT,
            publish_time TEXT,
            crawled_at TEXT,
            filtered_at TEXT
        )
    ''')
    
    # åˆ›å»ºç´¢å¼•
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_source_index_id ON pentosh1_news(source_index_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_id ON pentosh1_news(id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_url ON pentosh1_news(url)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_publish_time ON pentosh1_news(publish_time)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON pentosh1_news(source)')
    
    conn.commit()
    conn.close()


def load_news_batch(db_path: Path, offset: int, limit: int) -> List[Dict[str, Any]]:
    """ä»æºæ•°æ®åº“åŠ è½½ä¸€æ‰¹æ–°é—»"""
    if not db_path.exists():
        return []
    
    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT index_id, id, url, title, content, summary, source, publish_time, crawled_at
        FROM history_news
        WHERE content IS NOT NULL AND content != ''
        ORDER BY index_id ASC
        LIMIT ? OFFSET ?
    ''', (limit, offset))
    
    rows = cursor.fetchall()
    conn.close()
    
    news_list = []
    for row in rows:
        news_list.append({
            "index_id": row[0],
            "id": row[1],
            "url": row[2],
            "title": row[3],
            "content": row[4],
            "summary": row[5],
            "source": row[6],
            "publish_time": row[7],
            "crawled_at": row[8]
        })
    
    return news_list


def check_already_filtered(db_path: Path, index_id: int) -> bool:
    """æ£€æŸ¥æ–°é—»æ˜¯å¦å·²ç»è¢«ç­›é€‰è¿‡"""
    if not db_path.exists():
        return False
    
    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()
    
    cursor.execute('SELECT 1 FROM pentosh1_news WHERE source_index_id = ?', (index_id,))
    exists = cursor.fetchone() is not None
    
    conn.close()
    return exists


def filter_news_with_deepseek(news_batch: List[Dict[str, Any]], client: OpenAI) -> List[int]:
    """ä½¿ç”¨ DeepSeek API ç­›é€‰æ–°é—»æ‰¹æ¬¡
    
    Args:
        news_batch: æ–°é—»åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« id, content ç­‰å­—æ®µ
        client: DeepSeek OpenAI å®¢æˆ·ç«¯
    
    Returns:
        è¢«é€‰ä¸­çš„æ–°é—» index_id åˆ—è¡¨
    """
    if not news_batch:
        return []
    
    # æ„é€  User Prompt
    news_text_block = "\n".join([
        f"ID: {item['index_id']} | Date: {item.get('publish_time', 'N/A')} | Content: {item.get('content', item.get('summary', ''))[:500]}"
        for item in news_batch
    ])
    
    user_prompt = f"""
Here is the batch of news to filter. 
Select only the ones that match the Pentosh1 Macro/Flow strategy.

--- NEWS BATCH START ---
{news_text_block}
--- NEWS BATCH END ---
"""
    
    # å‘é€è¯·æ±‚ï¼ˆå¸¦é‡è¯•ï¼‰
    for attempt in range(MAX_RETRIES):
        try:
            response = client.chat.completions.create(
                model=DEEPSEEK_MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "json_object"},
                temperature=0.0
            )
            
            # è§£æç»“æœ
            result = json.loads(response.choices[0].message.content)
            selected_ids = result.get('selected_ids', [])
            
            # éªŒè¯è¿”å›çš„ ID æ˜¯å¦åœ¨æ‰¹æ¬¡ä¸­
            batch_index_ids = {item['index_id'] for item in news_batch}
            valid_selected_ids = [idx for idx in selected_ids if idx in batch_index_ids]
            
            return valid_selected_ids
            
        except json.JSONDecodeError as e:
            print(f"      âš ï¸  JSON è§£æå¤±è´¥: {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
                continue
            return []
        except Exception as e:
            print(f"      âš ï¸  API è°ƒç”¨å¤±è´¥: {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY * (attempt + 1))
                continue
            return []
    
    return []


def save_filtered_news(target_db_path: Path, news_items: List[Dict[str, Any]]):
    """ä¿å­˜ç­›é€‰åçš„æ–°é—»åˆ°ç›®æ ‡æ•°æ®åº“"""
    if not news_items:
        return
    
    conn = sqlite3.connect(str(target_db_path))
    cursor = conn.cursor()
    
    filtered_at = datetime.now().isoformat()
    
    for item in news_items:
        try:
            cursor.execute('''
                INSERT OR IGNORE INTO pentosh1_news 
                (source_index_id, id, url, title, content, summary, source, publish_time, crawled_at, filtered_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                item['index_id'],
                item['id'],
                item['url'],
                item['title'],
                item['content'],
                item['summary'],
                item['source'],
                item['publish_time'],
                item['crawled_at'],
                filtered_at
            ))
        except sqlite3.IntegrityError:
            # å¿½ç•¥é‡å¤è®°å½•
            pass
    
    conn.commit()
    conn.close()


def get_total_news_count(db_path: Path) -> int:
    """è·å–æºæ•°æ®åº“ä¸­çš„æ–°é—»æ€»æ•°"""
    if not db_path.exists():
        return 0
    
    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT COUNT(*) FROM history_news
        WHERE content IS NOT NULL AND content != ''
    ''')
    
    count = cursor.fetchone()[0]
    conn.close()
    
    return count


def get_already_filtered_count(target_db_path: Path) -> int:
    """è·å–å·²ç»ç­›é€‰è¿‡çš„æ–°é—»æ•°é‡"""
    if not target_db_path.exists():
        return 0
    
    conn = sqlite3.connect(str(target_db_path))
    cursor = conn.cursor()
    
    cursor.execute('SELECT COUNT(*) FROM pentosh1_news')
    count = cursor.fetchone()[0]
    conn.close()
    
    return count


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ” Pentosh1 æ–°é—»ç­›é€‰ç³»ç»Ÿ")
    print("=" * 80)
    print()
    
    # 1. æ£€æŸ¥ API Key
    if not DEEPSEEK_API_KEY:
        print("âŒ æœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        print("   è¯·è®¾ç½®: $env:DEEPSEEK_API_KEY='your_api_key'")
        return 1
    
    # 2. æ£€æŸ¥æºæ•°æ®åº“
    if not SOURCE_DB_PATH.exists():
        print(f"âŒ æºæ•°æ®åº“ä¸å­˜åœ¨: {SOURCE_DB_PATH}")
        return 1
    
    print(f"âœ… æºæ•°æ®åº“: {SOURCE_DB_PATH}")
    
    # 3. åˆå§‹åŒ–ç›®æ ‡æ•°æ®åº“
    print(f"ğŸ“‚ ç›®æ ‡æ•°æ®åº“: {TARGET_DB_PATH}")
    init_pentosh1_database(TARGET_DB_PATH)
    print("âœ… ç›®æ ‡æ•°æ®åº“å·²åˆå§‹åŒ–")
    print()
    
    # 4. åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
    print("ğŸ¤– åˆå§‹åŒ– DeepSeek API å®¢æˆ·ç«¯...")
    client = OpenAI(
        api_key=DEEPSEEK_API_KEY,
        base_url=DEEPSEEK_BASE_URL
    )
    print(f"   æ¨¡å‹: {DEEPSEEK_MODEL}")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print()
    
    # 5. ç»Ÿè®¡ä¿¡æ¯
    total_count = get_total_news_count(SOURCE_DB_PATH)
    already_filtered = get_already_filtered_count(TARGET_DB_PATH)
    
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æºæ•°æ®åº“æ€»æ–°é—»æ•°: {total_count:,}")
    print(f"   å·²ç­›é€‰æ–°é—»æ•°: {already_filtered:,}")
    print(f"   å¾…ç­›é€‰æ–°é—»æ•°: {total_count - already_filtered:,}")
    print()
    
    if total_count == 0:
        print("âŒ æºæ•°æ®åº“ä¸­æ²¡æœ‰æ–°é—»")
        return 1
    
    # 6. å¼€å§‹æ‰¹é‡ç­›é€‰
    print("=" * 80)
    print("ğŸš€ å¼€å§‹æ‰¹é‡ç­›é€‰...")
    print("=" * 80)
    print()
    
    offset = 0
    batch_num = 0
    total_selected = 0
    total_processed = 0
    
    start_time = time.time()
    
    while True:
        # åŠ è½½ä¸€æ‰¹æ–°é—»
        news_batch = load_news_batch(SOURCE_DB_PATH, offset, BATCH_SIZE)
        
        if not news_batch:
            break
        
        # è¿‡æ»¤æ‰å·²ç»ç­›é€‰è¿‡çš„æ–°é—»
        news_to_filter = [
            item for item in news_batch
            if not check_already_filtered(TARGET_DB_PATH, item['index_id'])
        ]
        
        if not news_to_filter:
            offset += BATCH_SIZE
            continue
        
        batch_num += 1
        print(f"ğŸ“¦ æ‰¹æ¬¡ #{batch_num}: å¤„ç† {len(news_to_filter)} æ¡æ–°é—» (ç´¢å¼• {offset} - {offset + len(news_batch) - 1})")
        
        # è°ƒç”¨ DeepSeek API ç­›é€‰
        selected_ids = filter_news_with_deepseek(news_to_filter, client)
        
        # ä¿å­˜ç­›é€‰ç»“æœ
        selected_news = [item for item in news_to_filter if item['index_id'] in selected_ids]
        if selected_news:
            save_filtered_news(TARGET_DB_PATH, selected_news)
            total_selected += len(selected_news)
            print(f"   âœ… é€‰ä¸­ {len(selected_news)} æ¡æ–°é—»")
        else:
            print(f"   â­ï¸  æœªé€‰ä¸­ä»»ä½•æ–°é—»")
        
        total_processed += len(news_to_filter)
        offset += BATCH_SIZE
        
        # æ˜¾ç¤ºè¿›åº¦
        progress = (total_processed / total_count) * 100 if total_count > 0 else 0
        print(f"   è¿›åº¦: {total_processed:,}/{total_count:,} ({progress:.1f}%) | å·²é€‰ä¸­: {total_selected:,}")
        print()
        
        # é¿å… API é™æµï¼Œæ·»åŠ å»¶è¿Ÿ
        time.sleep(1)
    
    elapsed_time = time.time() - start_time
    
    # 7. æœ€ç»ˆç»Ÿè®¡
    print("=" * 80)
    print("âœ… ç­›é€‰å®Œæˆï¼")
    print("=" * 80)
    print()
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   å¤„ç†æ‰¹æ¬¡: {batch_num}")
    print(f"   å¤„ç†æ–°é—»æ•°: {total_processed:,}")
    print(f"   é€‰ä¸­æ–°é—»æ•°: {total_selected:,}")
    print(f"   ç­›é€‰ç‡: {(total_selected/total_processed*100):.2f}%" if total_processed > 0 else "0%")
    print(f"   è€—æ—¶: {elapsed_time:.1f} ç§’")
    print(f"   å¹³å‡é€Ÿåº¦: {total_processed/elapsed_time:.1f} æ¡/ç§’" if elapsed_time > 0 else "N/A")
    print()
    print(f"ğŸ“‚ ç»“æœå·²ä¿å­˜åˆ°: {TARGET_DB_PATH}")
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())

