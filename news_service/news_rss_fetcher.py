"""
RSS æ–°é—»æŠ“å–ä¸æ¸…æ´—æ¨¡å—
ç”¨äºæŠ“å– Foresight News å’Œ BlockBeats çš„ RSS æºï¼Œå¹¶è¿›è¡Œ Pentosh1 ç­–ç•¥è¿‡æ»¤
"""
import feedparser
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
import re
import time
import os
import sys
import requests

# è®¾ç½® Windows æ§åˆ¶å°ç¼–ç ä¸º UTF-8
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


class CryptoNewsFetcher:
    def __init__(self):
        # å®šä¹‰ RSS æº - ä½¿ç”¨åˆ†ç±»æºï¼Œé¿å…å™ªéŸ³
        # åªè®¢é˜…é«˜ä»·å€¼åˆ†ç±»ï¼Œå‰”é™¤ Opinion/Analysis/Price Prediction
        self.feeds = {
            # --- ç›‘ç®¡ä¸å®è§‚ (S3 æ ¸å¿ƒ) ---
            "CD_Policy": [
                "https://www.coindesk.com/arc/outboundfeeds/rss/?path=/policy/"
            ],
            "CT_Regulation": [
                "https://cointelegraph.com/rss/category/policy-regulation"
            ],
            
            # --- æœºæ„ä¸èµ„é‡‘ (S1 æ ¸å¿ƒ) ---
            "CD_Business": [
                "https://www.coindesk.com/arc/outboundfeeds/rss/?path=/business/"
            ],
            "CT_Business": [
                "https://cointelegraph.com/rss/category/business"
            ],
            
            # --- æŠ€æœ¯ä¸åŸºæœ¬é¢ (S2 è½®åŠ¨) ---
            "CD_Tech": [
                "https://www.coindesk.com/arc/outboundfeeds/rss/?path=/tech/"
            ],
            "CT_Bitcoin": [
                "https://cointelegraph.com/rss/tag/bitcoin"
            ]
        }
        
        # Pentosh1 è®¤å¯çš„æ ‡ç­¾ (ç™½åå•)
        self.valid_tags = [
            "Business", "Regulation", "Policy", "Institutions", 
            "Bitcoin", "Ethereum", "Legal", "Adoption", "Technology",
            "Tech", "Business", "Policy", "Regulation"
        ]
        
        # å¿…é¡»å‰”é™¤çš„æ ‡ç­¾ (é»‘åå•)
        self.banned_tags = [
            "Market Analysis", "Price Analysis", "Opinion", 
            "Altcoin Watch", "NFT", "Metaverse", "Analysis",
            "Price Prediction", "Market Wrap", "Daily Digest"
        ]


    def clean_html(self, raw_html):
        """å»é™¤ RSS é‡Œçš„ HTML æ ‡ç­¾ (<p>, <a> ç­‰)"""
        if not raw_html:
            return ""
        soup = BeautifulSoup(raw_html, "html.parser")
        return soup.get_text().strip()

    def filter_for_pentosh1_strict(self, title, content, entry_tags=None):
        """
        é’ˆå¯¹ CoinDesk/CoinTelegraph çš„ä¸¥æ ¼è¿‡æ»¤å™¨
        ç›®æ ‡ï¼šåªä¿ç•™ç¡¬æ ¸äº‹å®ï¼Œå‰”é™¤åˆ†æå¸ˆççŒœ
        è¿”å›: (æ˜¯å¦ä¿ç•™, æ ‡ç­¾)
        """
        text = (title + " " + content).lower()
        
        # 1. åƒåœ¾å…³é”®è¯ (é»‘åå•å‡çº§ç‰ˆ)
        # CT/CD ç»å¸¸å‘ "Price Analysis", "Top 5 coins", "Why Bitcoin price is down"
        noise_keywords = [
            "price analysis", "price prediction", "top 5", "top 3", "could hit", 
            "opinion", "market wrap", "daily digest", "podcast", "video",
            "why", "what to expect", "bull run coming?", "analyst says",
            "should you buy", "when will", "how high", "could reach",
            "here's what happened", "what happened in crypto today", "daily",
            "depends on", "heavily depends", "shift to", "shifting to",
            "cycle", "end-of-year run", "$100k", "$100,000", "run to",
            "is bitcoin shifting", "shifting to a", "reveals how", "implications",
            "chance of hitting", "depends on investors", "market's response"
        ]
        
        # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«é—®å·ï¼ˆé€šå¸¸æ˜¯åˆ†æç±»æ–‡ç« ï¼‰
        if "?" in title:
            # ä½†å…è®¸ä¸€äº›ä¾‹å¤–ï¼Œæ¯”å¦‚ "Will SEC approve?" è¿™ç§ç¡¬æ–°é—»
            if not any(kw in text for kw in ["sec", "approve", "lawsuit", "ban", "jail"]):
                return False, "Analysis_Question"
        
        # æ£€æŸ¥æ˜æ˜¾çš„åˆ†æç±»æ ‡é¢˜æ¨¡å¼
        analysis_patterns = [
            "charts point", "point to", "direction of", "next move", "next big move",
            "risks return", "risks", "trader says", "makes sense", "price target"
        ]
        for pattern in analysis_patterns:
            if pattern in title.lower():
                # ä½†å…è®¸ä¸€äº›ä¾‹å¤–ï¼Œæ¯”å¦‚ç›‘ç®¡ç›¸å…³çš„ç¡¬æ–°é—»
                if not any(kw in text for kw in ["sec", "approve", "lawsuit", "ban", "jail", "regulation"]):
                    return False, "Analysis_Pattern"
        
        for noise in noise_keywords:
            if noise in text:
                return False, "Opinion/Noise"
        
        # 2. åŸºäº RSS æ ‡ç­¾çš„è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if entry_tags:
            # å…ˆæ£€æŸ¥æ˜¯ä¸æ˜¯åƒåœ¾åˆ†ç±»
            for tag in entry_tags:
                tag_lower = tag.lower()
                for banned in self.banned_tags:
                    if banned.lower() in tag_lower:
                        return False, "Banned_Tag"
        
        # 3. Pentosh1 æ ¸å¿ƒå…³æ³¨ (ç™½åå•)
        # åªä¿ç•™4å¤§ç±»æ ¸å¿ƒå…³é”®è¯ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼Œç¡®ä¿å•è¯è¾¹ç•Œï¼‰
        
        # 1. å®è§‚/è´¢æ”¿ (Macro/Fiscal) - å†³å®š"æ°´ä½"
        macro_keywords = [
            # æ ¸å¿ƒå¤®è¡Œä¸äººç‰©
            "fed", "federal reserve", "fomc", "jerome powell", "powell", "chair powell",
            "yellen", "janet yellen", "lagarde", "ecb", "european central bank",
            "boj", "bank of japan", "ueda", "pboc", "bank of england", "boe",
            "federal open market committee", "voting member", "fed governor",
            
            # åˆ©ç‡ä¸æ”¿ç­–åŠ¨ä½œ
            "rate cut", "rate hike", "interest rate", "fed funds rate", "benchmark rate",
            "basis points", "bps", "pivot", "pause", "skip", "hold rates",
            "tightening", "easing", "monetary policy", "hawkish", "dovish",
            "hike", "cut", "terminal rate", "dot plot", "neutral rate",
            "rate decision", "policy shift", "normalization", "cheap money",
            
            # é€šèƒ€ä¸ç»æµæŒ‡æ ‡
            "cpi", "consumer price index", "core cpi", "pce", "personal consumption expenditures",
            "ppi", "producer price index", "inflation", "deflation", "disinflation",
            "stagflation", "hyperinflation", "transitory", "sticky inflation",
            "nfp", "non-farm payrolls", "unemployment", "jobless claims", "labor market",
            "gdp", "gross domestic product", "recession", "soft landing", "hard landing",
            "economic slowdown", "economic growth", "pmi", "purchasing managers index",
            "retail sales", "consumer sentiment", "wage growth",
            
            # æµåŠ¨æ€§ä¸èµ„äº§è´Ÿå€ºè¡¨
            "liquidity", "global liquidity", "m2", "money supply", "balance sheet",
            "qe", "quantitative easing", "qt", "quantitative tightening", "tapering",
            "balance sheet reduction", "liquidity injection", "repo", "reverse repo", "rrp",
            "tga", "treasury general account", "bank reserves", "lending facility", "btfp",
            
            # å€ºåˆ¸ä¸ç¾å…ƒ
            "treasury", "us treasury", "bond", "yield", "yield curve", "inverted yield curve",
            "10-year", "2-year", "10y", "2y", "treasury yield", "sovereign debt",
            "dxy", "dollar index", "usd strength", "usd weakness", "fiat", "currency devaluation",
            "debt ceiling", "fiscal deficit", "government spending", "national debt", "credit rating"
        ]
        
        # 2. æœºæ„/èµ„é‡‘ (Smart Money) - å†³å®š"é£å‘"
        institutional_keywords = [
            # ETF ä¸ä¿¡æ‰˜äº§å“
            "etf", "spot etf", "bitcoin etf", "ethereum etf", "crypto etf", "etp", "etn",
            "gbtc", "ethe", "ibit", "fbtc", "arkb", "bitb", "trust", "nav discount",
            "premium", "conversion", "approval", "filing", "s-1", "19b-4",
            
            # é¡¶çº§èµ„ç®¡ä¸å‘è¡Œå•†
            "blackrock", "larry fink", "fidelity", "vanguard", "grayscale", "bitwise",
            "vaneck", "ark invest", "cathie wood", "franklin templeton", "wisdomtree",
            "invesco", "galaxy digital", "mike novogratz", "21shares", "valkyrie",
            "hashdex", "global x", "proshares", "direction",
            
            # æŠ•è¡Œä¸æ‰˜ç®¡
            "goldman", "goldman sachs", "jpmorgan", "jpm", "jamie dimon", "morgan stanley",
            "citi", "citigroup", "wells fargo", "bny mellon", "state street",
            "standard chartered", "nomura", "laser digital", "deutsche bank",
            "custody", "custodian", "prime broker", "institutional access",
            
            # ä¼ä¸šæŒä»“ä¸å·¨é²¸
            "microstrategy", "mstr", "michael saylor", "tesla", "elon musk", "block", "square",
            "metaplanet", "semler scientific", "corporate treasury", "balance sheet bitcoin",
            "whale", "accumulation", "dumping", "wallet movement", "dormant wallet",
            
            # äº¤æ˜“å•†ä¸åšå¸‚å•†
            "citadel", "jane street", "jump trading", "cumberland", "drw", "wintermute",
            "falconx", "genesis", "blockfi", "otc", "over-the-counter", "otc desk",
            "market maker", "liquidity provider",
            
            # èµ„é‡‘æµå‘ä¸è¡ç”Ÿå“
            "inflow", "outflow", "netflow", "net inflow", "net outflow", "aum",
            "assets under management", "volume", "trading volume", "open interest", "oi",
            "cme", "chicago mercantile exchange", "futures", "options", "longs", "shorts",
            "commitment of traders", "cot report", "funding rate", "basis", "contango", "backwardation"
        ]
        
        # 3. ç›‘ç®¡ (Regulation) - æœ€å¤§çš„é»‘å¤©é¹…
        regulation_keywords = [
            # ç¾å›½ç›‘ç®¡æœºæ„
            "sec", "securities and exchange commission", "gary gensler", "gensler", "hester peirce",
            "cftc", "commodity futures trading commission", "rostin behnam",
            "doj", "department of justice", "treasury", "yellen", "ofac", "fincen",
            "occ", "fdic", "irs", "white house", "congress", "senate", "house committee",
            
            # å…¨çƒç›‘ç®¡
            "mica", "markets in crypto-assets", "esma", "ecb", "eu parliament",
            "fca", "financial conduct authority", "uk treasury",
            "mas", "monetary authority of singapore", "sfc", "hong kong",
            "fsa", "japan", "vara", "dubai", "adgm", "abu dhabi",
            "fatf", "financial action task force", "g20", "imf", "bis",
            
            # æ³•å¾‹è¡ŒåŠ¨ä¸æœ¯è¯­
            "lawsuit", "sue", "sued", "suing", "legal action", "litigation", "court",
            "judge", "ruling", "verdict", "settlement", "settle", "fine", "penalty",
            "charges", "indictment", "subpoena", "wells notice", "enforcement action",
            "cease and desist", "injunction", "guilty", "appeal", "dismissal",
            
            # ç›‘ç®¡çŠ¶æ€ä¸åˆ†ç±»
            "approve", "approval", "reject", "rejection", "deny", "denial", "delay", "deadline",
            "ban", "banned", "prohibit", "crackdown", "restrict", "illegal",
            "security", "securities", "unregistered securities", "howey test", "investment contract",
            "commodity", "property", "currency", "legal tender", "asset class",
            
            # åˆè§„ä¸ç«‹æ³•
            "kyc", "aml", "anti-money laundering", "travel rule", "sanctions", "tornado cash",
            "privacy mixer", "compliance", "regulatory framework", "legislation", "bill",
            "fit21", "stablecoin bill", "sab 121", "custody rule", "license", "charter",
            "elizabeth warren", "cynthia lummis", "patrick mchenry", "tom emmer"
        ]
        
        # 4. é£é™©äº‹ä»¶ (Risk Events) - ç”¨äºé£æ§
        risk_keywords = [
            # é»‘å®¢ä¸æ”»å‡»
            "hack", "hacked", "hacker", "exploit", "exploited", "vulnerability", "bug",
            "attack", "attacker", "breach", "security breach", "compromised",
            "private key", "phishing", "scam", "fraud", "theft", "stolen",
            "bridge hack", "cross-chain hack", "smart contract exploit", "flash loan",
            "51% attack", "reorg", "double spend", "malware", "ransomware",
            
            # è´¢åŠ¡å´©æºƒä¸ç ´äº§
            "bankrupt", "bankruptcy", "chapter 11", "insolvent", "insolvency", "default",
            "collapse", "implode", "shutdown", "close down", "liquidate", "liquidation",
            "margin call", "underwater", "bad debt", "deficit", "hole in balance sheet",
            "restructuring", "receivership", "ftx", "alameda", "celsius", "voyager", "3ac",
            
            # å¸‚åœºè„±é”šä¸æš‚åœ
            "depeg", "de-peg", "lose peg", "unpeg", "stablecoin depeg", "usdt depeg", "usdc depeg",
            "halt", "halted", "suspended", "pause", "paused", "freeze", "frozen",
            "withdrawal", "withdrawals halted", "deposits suspended", "network congestion",
            "outage", "downtime", "offline", "delist", "delisting",
            
            # æ¬ºè¯ˆä¸çŠ¯ç½ª
            "ponzi", "pyramid scheme", "rug pull", "soft rug", "exit scam",
            "money laundering", "terrorist financing", "dark web", "silk road",
            "seized", "confiscated", "arrest", "arrested", "jail", "prison",
            "do kwon", "sbf", "sam bankman-fried", "mashinsky", "fraudster",
            
            # ç³»ç»Ÿæ€§é£é™©
            "contagion", "spillover", "systemic risk", "domino effect", "cascade",
            "black swan", "crash", "plunge", "dump", "capitulation", "panic selling",
            "fud", "fear uncertainty doubt", "bank run", "run on the bank"
        ]
        
        # æŒ‰ç±»åˆ«ç»„ç»‡å…³é”®è¯
        high_value_keywords = {
            "Macro": macro_keywords,
            "Institutional": institutional_keywords,
            "Regulation": regulation_keywords,
            "Risk": risk_keywords
        }
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼ˆå¸¦å•è¯è¾¹ç•Œï¼‰
        for tag, keywords in high_value_keywords.items():
            for kw in keywords:
                # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
                escaped_kw = re.escape(kw.lower())
                if ' ' in kw or '-' in kw:
                    # å¤šè¯å…³é”®è¯ï¼šå…è®¸ç©ºæ ¼ã€è¿å­—ç¬¦å’Œæ ‡ç‚¹
                    pattern = escaped_kw.replace(r'\ ', r'[\s\-]+').replace(r'\-', r'[\s\-]+')
                else:
                    # å•è¯è¾¹ç•ŒåŒ¹é…
                    pattern = r'\b' + escaped_kw + r'\b'
                
                if re.search(pattern, text, re.IGNORECASE):
                    return True, tag
        
        # 4. åŸºäº RSS æ ‡ç­¾çš„ç™½åå•æ£€æŸ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # æ³¨æ„ï¼šæ ‡ç­¾æ£€æŸ¥åªæ˜¯è¾…åŠ©ï¼Œä¸»è¦ä¾èµ–å…³é”®è¯åŒ¹é…
        # å¦‚æœæ ‡ç­¾åŒ¹é…åˆ°å…³é”®è¯ç±»åˆ«ï¼Œè¿”å›å¯¹åº”åˆ†ç±»
        if entry_tags:
            for tag in entry_tags:
                tag_lower = tag.lower()
                # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦åŒ…å«å…³é”®è¯
                if any(kw in tag_lower for kw in ["regulation", "policy", "legal", "sec", "lawsuit"]):
                    return True, "Regulation"
                elif any(kw in tag_lower for kw in ["business", "institution", "etf", "funding"]):
                    return True, "Institutional"
                elif any(kw in tag_lower for kw in ["macro", "fed", "inflation", "rate"]):
                    return True, "Macro"
                elif any(kw in tag_lower for kw in ["hack", "exploit", "bankrupt", "halt"]):
                    return True, "Risk"
        
        # é»˜è®¤ä¸¢å¼ƒ (åªä¿ç•™åŒ¹é…åˆ°4å¤§ç±»å…³é”®è¯çš„æ–°é—»)
        return False, "Low_Relevance"

    def fetch_all(self, limit=None):
        """
        æŠ“å–æ‰€æœ‰æ–°é—»æºçš„æœ€æ–°æ–°é—»ï¼ˆä½¿ç”¨ä¸¥æ ¼è¿‡æ»¤å™¨ï¼Œåªä¿ç•™ç¡¬æ–°é—»ï¼‰
        :param limit: é™åˆ¶è¿”å›çš„æ–°é—»æ•°é‡ï¼ŒNone è¡¨ç¤ºè¿”å›æ‰€æœ‰
        """
        all_news = []
        print(f"ğŸ“¡ å¼€å§‹æŠ“å– RSS æº: {datetime.now()}")

        for source_name, urls in self.feeds.items():
            # æ”¯æŒå¤šä¸ªå¤‡ç”¨ URL
            if isinstance(urls, str):
                urls = [urls]
            
            feed = None
            last_error = None
            
            for url in urls:
                try:
                    print(f"   ... æ­£åœ¨è¿æ¥ {source_name}: {url}")
                    # ä½¿ç”¨ requests è·å–å†…å®¹ï¼Œè®¾ç½® User-Agent
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                    }
                    response = requests.get(url, headers=headers, timeout=10)
                    response.encoding = response.apparent_encoding or 'utf-8'
                    
                    # è°ƒè¯•ï¼šæ£€æŸ¥å“åº”å†…å®¹
                    if response.status_code != 200:
                        print(f"   âš ï¸ HTTP çŠ¶æ€ç : {response.status_code}")
                        continue
                    
                    # æ£€æŸ¥å†…å®¹ç±»å‹
                    content_type = response.headers.get('Content-Type', '')
                    if 'xml' not in content_type.lower() and 'rss' not in content_type.lower() and 'atom' not in content_type.lower():
                        print(f"   âš ï¸ å†…å®¹ç±»å‹å¯èƒ½ä¸æ­£ç¡®: {content_type}")
                    
                    # ä½¿ç”¨ feedparser è§£æå†…å®¹
                    feed = feedparser.parse(response.content)

                    # å³ä½¿æœ‰è­¦å‘Šï¼Œä¹Ÿå°è¯•è¯»å–æ¡ç›®ï¼ˆæœ‰äº› RSS æºæ ¼å¼ä¸å®Œç¾ä½†ä»å¯ç”¨ï¼‰
                    if feed.bozo and len(feed.entries) == 0:
                        error_msg = ""
                        if hasattr(feed, 'bozo_exception'):
                            error_msg = f" ({feed.bozo_exception})"
                        print(f"   âš ï¸ {source_name} RSS è§£æå¤±è´¥{error_msg}ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæº...")
                        last_error = feed.bozo_exception if hasattr(feed, 'bozo_exception') else "è§£æé”™è¯¯"
                        continue
                    
                    # å¦‚æœæˆåŠŸè·å–åˆ°æ¡ç›®ï¼Œå³ä½¿æœ‰è­¦å‘Šä¹Ÿä½¿ç”¨
                    if len(feed.entries) > 0:
                        if feed.bozo:
                            print(f"   âš ï¸ {source_name} RSS æœ‰æ ¼å¼è­¦å‘Šï¼Œä½†å·²è·å–åˆ° {len(feed.entries)} æ¡æ–°é—»")
                        break
                    else:
                        print(f"   âš ï¸ {source_name} æœªè·å–åˆ°æ–°é—»æ¡ç›®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæº...")
                        continue
                    
                except Exception as e:
                    print(f"   âš ï¸ {source_name} è¿æ¥å¤±è´¥ ({url}): {e}")
                    last_error = str(e)
                    continue

            # å¦‚æœæ‰€æœ‰æºéƒ½å¤±è´¥äº†
            if feed is None or len(feed.entries) == 0:
                print(f"   âŒ {source_name} æ‰€æœ‰æºå‡å¤±è´¥ï¼Œè·³è¿‡")
                continue

            # æˆåŠŸè·å– feedï¼Œå¼€å§‹å¤„ç†æ¡ç›®
            print(f"   âœ… {source_name} è¿æ¥æˆåŠŸï¼Œè·å–åˆ° {len(feed.entries)} æ¡æ–°é—»")
            try:
                for entry in feed.entries:
                    # æå–åŸºç¡€ä¿¡æ¯
                    title = entry.title
                    # æœ‰äº› RSS çš„æ­£æ–‡åœ¨ 'summary'ï¼Œæœ‰äº›åœ¨ 'content'ï¼Œæœ‰äº›åœ¨ 'description'
                    raw_content = entry.get('summary', '') or entry.get('description', '')

                    # å¤„ç† content å­—æ®µï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨ï¼‰
                    if isinstance(raw_content, list) and len(raw_content) > 0:
                        raw_content = raw_content[0].get('value', '') if isinstance(raw_content[0], dict) else str(raw_content[0])
                    elif not isinstance(raw_content, str):
                        raw_content = str(raw_content)

                    content = self.clean_html(raw_content)

                    # æå–é“¾æ¥
                    link = entry.link

                    # æå– RSS æ ‡ç­¾
                    entry_tags = []
                    if hasattr(entry, 'tags'):
                        entry_tags = [t.term for t in entry.tags]

                    # å¤„ç†æ—¶é—´ (æ ‡å‡†åŒ–ä¸º YYYY-MM-DD HH:MM:SS)
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_time = datetime(*entry.published_parsed[:6])
                    else:
                        pub_time = datetime.now()

                    # ğŸ”¥ æ ¸å¿ƒï¼šPentosh1 ä¸¥æ ¼è¿‡æ»¤å™¨ï¼ˆåªä¿ç•™ç¡¬æ–°é—»ï¼‰
                    keep, tag = self.filter_for_pentosh1_strict(title, content, entry_tags=entry_tags)

                    if keep:
                        all_news.append({
                            "source": source_name,
                            "time": pub_time,
                            "tag": tag,
                            "title": title,
                            "content_summary": content[:500],  # æˆªå–å‰500å­—ç»™LLM
                            "url": link,
                            "rss_tags": ", ".join(entry_tags) if entry_tags else ""
                        })
            except Exception as e:
                print(f"   âŒ {source_name} å¤„ç†æ–°é—»æ¡ç›®æ—¶å‡ºé”™: {e}")

        # è½¬ä¸º DataFrame å¹¶æŒ‰æ—¶é—´å€’åº
        df = pd.DataFrame(all_news)
        if not df.empty:
            df = df.sort_values(by="time", ascending=False)
            
            # ğŸ”¥ å»é‡ï¼šç›¸åŒæ—¶é—´+æ ‡é¢˜åªä¿ç•™ä¸€æ¡ï¼Œåˆå¹¶æ‰€æœ‰tag
            print(f"ğŸ“Š å»é‡å‰: {len(df)} æ¡æ–°é—»")
            df = self._deduplicate_news(df)
            print(f"ğŸ“Š å»é‡å: {len(df)} æ¡æ–°é—»")
            
            # å¦‚æœæŒ‡å®šäº†é™åˆ¶ï¼Œåªè¿”å›å‰ N æ¡
            if limit is not None and len(df) > limit:
                df = df.head(limit)
            print(f"âœ… æŠ“å–å®Œæˆï¼å…±è·å¾— {len(df)} æ¡é«˜ä»·å€¼æ–°é—»ã€‚")
        else:
            print("âš ï¸ æœªè·å–åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°é—»ã€‚")

        return df
    
    def _deduplicate_news(self, df):
        """
        å»é‡ï¼šç›¸åŒæ—¶é—´+æ ‡é¢˜åªä¿ç•™ä¸€æ¡ï¼Œåˆå¹¶æ‰€æœ‰tagå’Œsource
        """
        if df.empty:
            return df
        
        # åˆ›å»ºå”¯ä¸€æ ‡è¯†ï¼šæ—¶é—´ + æ ‡é¢˜
        df['time_str'] = df['time'].astype(str)
        df['unique_key'] = df['time_str'] + '|||' + df['title']
        
        # ç”¨äºå­˜å‚¨å»é‡åçš„æ•°æ®
        deduplicated_rows = []
        seen_keys = {}
        
        for idx, row in df.iterrows():
            key = row['unique_key']
            
            if key not in seen_keys:
                # ç¬¬ä¸€æ¬¡é‡åˆ°è¿™æ¡æ–°é—»ï¼Œç›´æ¥æ·»åŠ 
                seen_keys[key] = len(deduplicated_rows)
                deduplicated_rows.append({
                    "source": row['source'],
                    "time": row['time'],
                    "tag": row['tag'],
                    "title": row['title'],
                    "content_summary": row['content_summary'],
                    "url": row['url'],
                    "rss_tags": row['rss_tags']
                })
            else:
                # é‡å¤æ–°é—»ï¼Œåˆå¹¶tagã€sourceå’Œrss_tags
                existing_idx = seen_keys[key]
                existing = deduplicated_rows[existing_idx]
                
                # åˆå¹¶sourceï¼ˆç”¨é€—å·åˆ†éš”ï¼Œå»é‡ï¼‰
                existing_sources = set(existing['source'].split(", "))
                new_sources = set([row['source']])
                merged_sources = existing_sources | new_sources
                existing['source'] = ", ".join(sorted(merged_sources))
                
                # åˆå¹¶tagï¼ˆç”¨é€—å·åˆ†éš”ï¼Œå»é‡ï¼‰
                existing_tags = set(existing['tag'].split(", "))
                new_tags = set([row['tag']])
                merged_tags = existing_tags | new_tags
                existing['tag'] = ", ".join(sorted(merged_tags))
                
                # åˆå¹¶rss_tagsï¼ˆå»é‡ååˆå¹¶ï¼‰
                existing_rss_tags = set(existing['rss_tags'].split(", ") if existing['rss_tags'] else [])
                new_rss_tags = set(row['rss_tags'].split(", ") if row['rss_tags'] else [])
                merged_rss_tags = existing_rss_tags | new_rss_tags
                existing['rss_tags'] = ", ".join(sorted([t for t in merged_rss_tags if t]))  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        
        # è½¬æ¢å›DataFrameå¹¶åˆ é™¤è¾…åŠ©åˆ—
        result_df = pd.DataFrame(deduplicated_rows)
        if not result_df.empty:
            result_df = result_df.sort_values(by="time", ascending=False)
        
        return result_df


if __name__ == "__main__":
    fetcher = CryptoNewsFetcher()
    # æŠ“å–æœ€è¿‘åæ¡ç¡¬æ–°é—»ï¼ˆä½¿ç”¨ä¸¥æ ¼è¿‡æ»¤å™¨ï¼Œå‰”é™¤ Opinion/Analysisï¼‰
    df = fetcher.fetch_all(limit=10)

    if not df.empty:
        # æ‰“å°æ‰€æœ‰æŠ“å–çš„æ–°é—»
        print(f"\n--- æœ€è¿‘ {len(df)} æ¡é«˜ä»·å€¼å¿«è®¯ ---")
        for idx, (i, row) in enumerate(df.iterrows(), 1):
            print(f"\n[{idx}] [{row['tag']}] {row['source']} | {row['time']}")
            print(f"æ ‡é¢˜: {row['title']}")
            print(f"æ‘˜è¦: {row['content_summary'][:100]}...")
            print(f"é“¾æ¥: {row['url']}")

        # ä¿å­˜ï¼Œå‡†å¤‡å–‚ç»™ DeepSeek åšé€»è¾‘æå–
        output_path = "pentosh1_news_feed.csv"
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    else:
        print("âš ï¸ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ–°é—»ï¼Œæœªç”Ÿæˆ CSV æ–‡ä»¶ã€‚")

