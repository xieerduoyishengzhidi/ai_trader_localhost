"""
æµ‹è¯•æ•°æ®åº“åŠŸèƒ½
"""
import sys
import sqlite3
import pandas as pd
from datetime import datetime
import os

# è®¾ç½® Windows æ§åˆ¶å°ç¼–ç ä¸º UTF-8
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

db_path = "test_history_news.db"

print("ğŸ§ª æµ‹è¯•æ•°æ®åº“åŠŸèƒ½\n")

# 1. åˆå§‹åŒ–æ•°æ®åº“
print("1ï¸âƒ£ åˆå§‹åŒ–æ•°æ®åº“...")
conn = sqlite3.connect(db_path)
cursor = conn.cursor()

cursor.execute('''
    CREATE TABLE IF NOT EXISTS history_news (
        id TEXT PRIMARY KEY,
        url TEXT UNIQUE NOT NULL,
        title TEXT,
        content TEXT,
        summary TEXT,
        source TEXT,
        publish_time TEXT,
        crawled_at TEXT
    )
''')

cursor.execute('CREATE INDEX IF NOT EXISTS idx_url ON history_news(url)')
cursor.execute('CREATE INDEX IF NOT EXISTS idx_publish_time ON history_news(publish_time)')

conn.commit()
print(f"   âœ… æ•°æ®åº“è¡¨å·²åˆ›å»º")

# 2. æ’å…¥æµ‹è¯•æ•°æ®
print("\n2ï¸âƒ£ æ’å…¥æµ‹è¯•æ•°æ®...")
test_articles = [
    {
        "id": "abc123",
        "url": "https://test.com/1",
        "title": "Test Article 1",
        "content": "This is a test article content...",
        "summary": "This is a test article summary.",
        "source": "CoinTelegraph",
        "publish_time": str(datetime.now()),
        "crawled_at": str(datetime.now())
    },
    {
        "id": "def456",
        "url": "https://test.com/2",
        "title": "Test Article 2",
        "content": "Another test article content...",
        "summary": "Another test article summary.",
        "source": "CoinTelegraph",
        "publish_time": str(datetime.now()),
        "crawled_at": str(datetime.now())
    }
]

for article in test_articles:
    cursor.execute('''
        INSERT OR REPLACE INTO history_news 
        (id, url, title, content, summary, source, publish_time, crawled_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        article['id'],
        article['url'],
        article['title'],
        article['content'],
        article['summary'],
        article['source'],
        article['publish_time'],
        article['crawled_at']
    ))

conn.commit()
print(f"   âœ… å·²æ’å…¥ {len(test_articles)} æ¡æµ‹è¯•æ•°æ®")

# 3. æŸ¥è¯¢æ•°æ®
print("\n3ï¸âƒ£ æŸ¥è¯¢æ•°æ®...")
cursor.execute('SELECT COUNT(*) FROM history_news')
count = cursor.fetchone()[0]
print(f"   âœ… æ•°æ®åº“ä¸­å…±æœ‰ {count} æ¡è®°å½•")

cursor.execute('SELECT url FROM history_news')
urls = cursor.fetchall()
print(f"   ğŸ“‹ URLåˆ—è¡¨:")
for url in urls:
    print(f"      - {url[0]}")

# 4. ä½¿ç”¨pandasè¯»å–
print("\n4ï¸âƒ£ ä½¿ç”¨pandasè¯»å–æ•°æ®...")
df = pd.read_sql_query('SELECT * FROM history_news', conn)
print(f"   âœ… è¯»å–æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•")
print(f"\n   DataFrameç»“æ„:")
print(df.head())

# 5. æ¸…ç†
print("\n5ï¸âƒ£ æ¸…ç†æµ‹è¯•æ•°æ®åº“...")
conn.close()
if os.path.exists(db_path):
    os.remove(db_path)
    print(f"   âœ… å·²åˆ é™¤ {db_path}")

print("\nâœ… æµ‹è¯•å®Œæˆï¼")

