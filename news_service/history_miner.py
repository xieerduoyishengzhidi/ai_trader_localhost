"""
å†å²æ–°é—»æŒ–æ˜æ¨¡å— - ä½¿ç”¨ Sitemap æŒ–æ˜æ³•
ç”¨äºæ„å»º Pentosh1 å†å²é€»è¾‘åº“ï¼ˆLogic DBï¼‰
"""
import feedparser
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
import time
import os
import sys
import requests
import xml.etree.ElementTree as ET
from urllib.parse import urlparse
import trafilatura
import hashlib
import sqlite3

# è®¾ç½® Windows æ§åˆ¶å°ç¼–ç ä¸º UTF-8
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


class HistoryNewsMiner:
    def __init__(self):
        # Pentosh1 å…³æ³¨çš„å…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤å†å²æ–‡ç« ï¼‰
        # åªä¿ç•™æ ¸å¿ƒå…³é”®è¯ï¼Œåˆ†ä¸º4å¤§ç±»
        
        # 1. å®è§‚/è´¢æ”¿ (Macro/Fiscal) - å†³å®š"æ°´ä½"
        macro_keywords = [
            # æ ¸å¿ƒå¤®è¡Œä¸äººç‰©
            "fed", "federal reserve", "fomc", "jerome powell", "powell", "chair powell",
            "yellen", "janet yellen", "lagarde", "ecb", "european central bank",
            "boj", "bank of japan", "ueda", "pboc", "bank of england", "boe",
            "federal open market committee", "voting member", "fed governor",
            
            # åˆ©ç‡ä¸æ”¿ç­–åŠ¨ä½œ
            "rate cut", "rate hike", "interest rate", "fed funds rate", "benchmark rate",
            "basis points", "bps", "pivot", "pause", "skip", "hold rates",
            "tightening", "easing", "monetary policy", "hawkish", "dovish",
            "hike", "cut", "terminal rate", "dot plot", "neutral rate",
            "rate decision", "policy shift", "normalization", "cheap money",
            
            # é€šèƒ€ä¸ç»æµæŒ‡æ ‡
            "cpi", "consumer price index", "core cpi", "pce", "personal consumption expenditures",
            "ppi", "producer price index", "inflation", "deflation", "disinflation",
            "stagflation", "hyperinflation", "transitory", "sticky inflation",
            "nfp", "non-farm payrolls", "unemployment", "jobless claims", "labor market",
            "gdp", "gross domestic product", "recession", "soft landing", "hard landing",
            "economic slowdown", "economic growth", "pmi", "purchasing managers index",
            "retail sales", "consumer sentiment", "wage growth",
            
            # æµåŠ¨æ€§ä¸èµ„äº§è´Ÿå€ºè¡¨
            "liquidity", "global liquidity", "m2", "money supply", "balance sheet",
            "qe", "quantitative easing", "qt", "quantitative tightening", "tapering",
            "balance sheet reduction", "liquidity injection", "repo", "reverse repo", "rrp",
            "tga", "treasury general account", "bank reserves", "lending facility", "btfp",
            
            # å€ºåˆ¸ä¸ç¾å…ƒ
            "treasury", "us treasury", "bond", "yield", "yield curve", "inverted yield curve",
            "10-year", "2-year", "10y", "2y", "treasury yield", "sovereign debt",
            "dxy", "dollar index", "usd strength", "usd weakness", "fiat", "currency devaluation",
            "debt ceiling", "fiscal deficit", "government spending", "national debt", "credit rating"
        ]
        
        # 2. æœºæ„/èµ„é‡‘ (Smart Money) - å†³å®š"é£å‘"
        institutional_keywords = [
            # ETF ä¸ä¿¡æ‰˜äº§å“
            "etf", "spot etf", "bitcoin etf", "ethereum etf", "crypto etf", "etp", "etn",
            "gbtc", "ethe", "ibit", "fbtc", "arkb", "bitb", "trust", "nav discount",
            "premium", "conversion", "approval", "filing", "s-1", "19b-4",
            
            # é¡¶çº§èµ„ç®¡ä¸å‘è¡Œå•†
            "blackrock", "larry fink", "fidelity", "vanguard", "grayscale", "bitwise",
            "vaneck", "ark invest", "cathie wood", "franklin templeton", "wisdomtree",
            "invesco", "galaxy digital", "mike novogratz", "21shares", "valkyrie",
            "hashdex", "global x", "proshares", "direction",
            
            # æŠ•è¡Œä¸æ‰˜ç®¡
            "goldman", "goldman sachs", "jpmorgan", "jpm", "jamie dimon", "morgan stanley",
            "citi", "citigroup", "wells fargo", "bny mellon", "state street",
            "standard chartered", "nomura", "laser digital", "deutsche bank",
            "custody", "custodian", "prime broker", "institutional access",
            
            # ä¼ä¸šæŒä»“ä¸å·¨é²¸
            "microstrategy", "mstr", "michael saylor", "tesla", "elon musk", "block", "square",
            "metaplanet", "semler scientific", "corporate treasury", "balance sheet bitcoin",
            "whale", "accumulation", "dumping", "wallet movement", "dormant wallet",
            
            # äº¤æ˜“å•†ä¸åšå¸‚å•†
            "citadel", "jane street", "jump trading", "cumberland", "drw", "wintermute",
            "falconx", "genesis", "blockfi", "otc", "over-the-counter", "otc desk",
            "market maker", "liquidity provider",
            
            # èµ„é‡‘æµå‘ä¸è¡ç”Ÿå“
            "inflow", "outflow", "netflow", "net inflow", "net outflow", "aum",
            "assets under management", "volume", "trading volume", "open interest", "oi",
            "cme", "chicago mercantile exchange", "futures", "options", "longs", "shorts",
            "commitment of traders", "cot report", "funding rate", "basis", "contango", "backwardation"
        ]
        
        # 3. ç›‘ç®¡ (Regulation) - æœ€å¤§çš„é»‘å¤©é¹…
        regulation_keywords = [
            # ç¾å›½ç›‘ç®¡æœºæ„
            "sec", "securities and exchange commission", "gary gensler", "gensler", "hester peirce",
            "cftc", "commodity futures trading commission", "rostin behnam",
            "doj", "department of justice", "treasury", "yellen", "ofac", "fincen",
            "occ", "fdic", "irs", "white house", "congress", "senate", "house committee",
            
            # å…¨çƒç›‘ç®¡
            "mica", "markets in crypto-assets", "esma", "ecb", "eu parliament",
            "fca", "financial conduct authority", "uk treasury",
            "mas", "monetary authority of singapore", "sfc", "hong kong",
            "fsa", "japan", "vara", "dubai", "adgm", "abu dhabi",
            "fatf", "financial action task force", "g20", "imf", "bis",
            
            # æ³•å¾‹è¡ŒåŠ¨ä¸æœ¯è¯­
            "lawsuit", "sue", "sued", "suing", "legal action", "litigation", "court",
            "judge", "ruling", "verdict", "settlement", "settle", "fine", "penalty",
            "charges", "indictment", "subpoena", "wells notice", "enforcement action",
            "cease and desist", "injunction", "guilty", "appeal", "dismissal",
            
            # ç›‘ç®¡çŠ¶æ€ä¸åˆ†ç±»
            "approve", "approval", "reject", "rejection", "deny", "denial", "delay", "deadline",
            "ban", "banned", "prohibit", "crackdown", "restrict", "illegal",
            "security", "securities", "unregistered securities", "howey test", "investment contract",
            "commodity", "property", "currency", "legal tender", "asset class",
            
            # åˆè§„ä¸ç«‹æ³•
            "kyc", "aml", "anti-money laundering", "travel rule", "sanctions", "tornado cash",
            "privacy mixer", "compliance", "regulatory framework", "legislation", "bill",
            "fit21", "stablecoin bill", "sab 121", "custody rule", "license", "charter",
            "elizabeth warren", "cynthia lummis", "patrick mchenry", "tom emmer"
        ]
        
        # 4. é£é™©äº‹ä»¶ (Risk Events) - ç”¨äºé£æ§
        risk_keywords = [
            # é»‘å®¢ä¸æ”»å‡»
            "hack", "hacked", "hacker", "exploit", "exploited", "vulnerability", "bug",
            "attack", "attacker", "breach", "security breach", "compromised",
            "private key", "phishing", "scam", "fraud", "theft", "stolen",
            "bridge hack", "cross-chain hack", "smart contract exploit", "flash loan",
            "51% attack", "reorg", "double spend", "malware", "ransomware",
            
            # è´¢åŠ¡å´©æºƒä¸ç ´äº§
            "bankrupt", "bankruptcy", "chapter 11", "insolvent", "insolvency", "default",
            "collapse", "implode", "shutdown", "close down", "liquidate", "liquidation",
            "margin call", "underwater", "bad debt", "deficit", "hole in balance sheet",
            "restructuring", "receivership", "ftx", "alameda", "celsius", "voyager", "3ac",
            
            # å¸‚åœºè„±é”šä¸æš‚åœ
            "depeg", "de-peg", "lose peg", "unpeg", "stablecoin depeg", "usdt depeg", "usdc depeg",
            "halt", "halted", "suspended", "pause", "paused", "freeze", "frozen",
            "withdrawal", "withdrawals halted", "deposits suspended", "network congestion",
            "outage", "downtime", "offline", "delist", "delisting",
            
            # æ¬ºè¯ˆä¸çŠ¯ç½ª
            "ponzi", "pyramid scheme", "rug pull", "soft rug", "exit scam",
            "money laundering", "terrorist financing", "dark web", "silk road",
            "seized", "confiscated", "arrest", "arrested", "jail", "prison",
            "do kwon", "sbf", "sam bankman-fried", "mashinsky", "fraudster",
            
            # ç³»ç»Ÿæ€§é£é™©
            "contagion", "spillover", "systemic risk", "domino effect", "cascade",
            "black swan", "crash", "plunge", "dump", "capitulation", "panic selling",
            "fud", "fear uncertainty doubt", "bank run", "run on the bank"
        ]
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        self.target_keywords = macro_keywords + institutional_keywords + regulation_keywords + risk_keywords
        
        # æ–°é—»ç«™ç‚¹é…ç½®
        self.sites = {
            "CoinTelegraph": {
                "sitemap": "https://cointelegraph.com/sitemap.xml",
                "base_url": "https://cointelegraph.com",
                "news_pattern": r"/news/"
            }
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def fetch_sitemap(self, sitemap_url):
        """è·å–å¹¶è§£æ sitemap.xml"""
        try:
            print(f"   ğŸ“ è·å–ç«™ç‚¹åœ°å›¾: {sitemap_url}")
            response = requests.get(sitemap_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # è§£æ XMLï¼ˆå¤„ç†å‘½åç©ºé—´ï¼‰
            try:
                root = ET.fromstring(response.content)
            except ET.ParseError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç”¨ BeautifulSoup
                soup = BeautifulSoup(response.content, 'xml')
                urls = []
                for loc in soup.find_all('loc'):
                    urls.append(loc.text)
                return urls
            
            # å®šä¹‰å‘½åç©ºé—´
            namespaces = {
                'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9',
                'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'
            }
            
            urls = []
            
            # æ–¹æ³•1: ä½¿ç”¨å‘½åç©ºé—´æŸ¥æ‰¾
            for namespace in ['{http://www.sitemaps.org/schemas/sitemap/0.9}', '']:
                for loc in root.findall(f'.//{namespace}loc'):
                    if loc.text:
                        urls.append(loc.text)
                if urls:
                    break
            
            # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾æ‰€æœ‰ loc æ ‡ç­¾
            if not urls:
                for elem in root.iter():
                    tag = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
                    if tag.lower() == 'loc' and elem.text:
                        urls.append(elem.text)
            
            return urls
        except Exception as e:
            print(f"   âŒ è·å–ç«™ç‚¹åœ°å›¾å¤±è´¥: {e}")
            return []

    def extract_monthly_sitemaps(self, main_sitemap_url):
        """ä»ä¸» sitemap æå–æœˆåº¦ sitemap é“¾æ¥"""
        print(f"ğŸ“¡ è§£æä¸»ç«™ç‚¹åœ°å›¾: {main_sitemap_url}")
        sitemaps = self.fetch_sitemap(main_sitemap_url)
        
        # è¿‡æ»¤å‡ºæœˆåº¦ sitemapï¼ˆé€šå¸¸åŒ…å« post-YYYY-MM æ ¼å¼ï¼‰
        monthly_sitemaps = []
        for sitemap_url in sitemaps:
            if 'post-' in sitemap_url.lower() or 'sitemap' in sitemap_url.lower():
                monthly_sitemaps.append(sitemap_url)
        
        print(f"   âœ… æ‰¾åˆ° {len(monthly_sitemaps)} ä¸ªæœˆåº¦ç«™ç‚¹åœ°å›¾")
        return monthly_sitemaps

    def filter_news_urls(self, urls, keywords=None, months_back=12):
        """è¿‡æ»¤æ–°é—» URLï¼Œåªä¿ç•™åŒ…å«å…³é”®è¯çš„é“¾æ¥ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼‰"""
        if keywords is None:
            keywords = self.target_keywords
        
        filtered_urls = []
        cutoff_date = datetime.now() - timedelta(days=months_back * 30)
        
        # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆä½¿ç”¨å•è¯è¾¹ç•Œï¼‰
        # å°†å…³é”®è¯è½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼ï¼Œä½¿ç”¨ \b ç¡®ä¿å•è¯è¾¹ç•ŒåŒ¹é…
        patterns = []
        for kw in keywords:
            # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
            escaped_kw = re.escape(kw.lower())
            # ä½¿ç”¨å•è¯è¾¹ç•Œï¼Œä½†å…è®¸ç©ºæ ¼å’Œè¿å­—ç¬¦
            if ' ' in kw or '-' in kw:
                # å¤šè¯å…³é”®è¯ï¼šå…è®¸ç©ºæ ¼å’Œè¿å­—ç¬¦
                pattern = escaped_kw.replace(r'\ ', r'[\s-]+').replace(r'\-', r'[\s-]+')
            else:
                # å•è¯è¾¹ç•ŒåŒ¹é…
                pattern = r'\b' + escaped_kw + r'\b'
            patterns.append(pattern)
        
        # ç»„åˆæ‰€æœ‰æ¨¡å¼
        combined_pattern = '|'.join(patterns)
        regex = re.compile(combined_pattern, re.IGNORECASE)
        
        for url in urls:
            # åªä¿ç•™ /news/ ç±»å‹çš„é“¾æ¥
            if '/news/' not in url.lower():
                continue
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
            if regex.search(url.lower()):
                filtered_urls.append(url)
        
        return filtered_urls

    def extract_article_content(self, url):
        """ä½¿ç”¨ trafilatura æå–æ–‡ç« æ­£æ–‡ï¼ˆå®Œæ•´å†…å®¹ï¼Œä¸æˆªæ–­ï¼‰"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨ trafilatura ç›´æ¥ä» URL æå–
            article = trafilatura.extract(trafilatura.fetch_url(url))
            if article and len(article.strip()) > 100:
                return article.strip()  # è¿”å›å®Œæ•´å†…å®¹
        except Exception as e:
            pass
        
        # æ–¹æ³•2: å¤‡ç”¨æ–¹æ¡ˆ - requests + BeautifulSoup
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = response.apparent_encoding or 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨æå–æ­£æ–‡
            content_selectors = [
                'article',
                '[class*="article"]',
                '[class*="post-content"]',
                '[class*="content"]',
                'main'
            ]
            
            text_parts = []
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    for elem in elements[:2]:  # åªå–å‰2ä¸ª
                        text = elem.get_text(separator=' ', strip=True)
                        if len(text) > 200:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
                            text_parts.append(text)
                    if text_parts:
                        break
            
            if text_parts:
                full_text = ' '.join(text_parts)
                if len(full_text) > 100:
                    return full_text  # è¿”å›å®Œæ•´å†…å®¹
            
            return None
        except Exception as e:
            return None
    
    def extract_article_metadata(self, url, content):
        """ä»æ–‡ç« å†…å®¹æˆ– URL æå–å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€å‘å¸ƒæ—¶é—´ç­‰ï¼‰"""
        title = ""
        publish_time = None
        
        # å°è¯•ä» URL æå–æ ‡é¢˜
        url_parts = url.split('/')
        if url_parts:
            title = url_parts[-1].replace('-', ' ').replace('_', ' ').title()
        
        # å°è¯•ä»å†…å®¹ä¸­æå–æ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´
        if content:
            # ä½¿ç”¨ trafilatura æå–å…ƒæ•°æ®
            try:
                downloaded = trafilatura.fetch_url(url)
                if downloaded:
                    metadata = trafilatura.extract_metadata(downloaded)
                    if metadata:
                        if metadata.title:
                            title = metadata.title
                        if metadata.date:
                            try:
                                publish_time = datetime.fromisoformat(str(metadata.date).replace('Z', '+00:00'))
                            except:
                                pass
            except:
                pass
            
            # å¦‚æœ trafilatura å¤±è´¥ï¼Œå°è¯•ä» HTML ä¸­æå–
            if not publish_time:
                try:
                    response = requests.get(url, headers=self.headers, timeout=10)
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æå–æ ‡é¢˜
                    if not title or title == url_parts[-1]:
                        title_tag = soup.find('title')
                        if title_tag:
                            title = title_tag.get_text(strip=True)
                    
                    # æå–å‘å¸ƒæ—¶é—´
                    time_selectors = [
                        'time[datetime]',
                        '[class*="date"]',
                        '[class*="time"]',
                        'meta[property="article:published_time"]',
                        'meta[name="publish-date"]'
                    ]
                    for selector in time_selectors:
                        elem = soup.select_one(selector)
                        if elem:
                            time_str = elem.get('datetime') or elem.get('content') or elem.get_text(strip=True)
                            if time_str:
                                try:
                                    # å°è¯•è§£æå„ç§æ—¶é—´æ ¼å¼
                                    publish_time = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                                    break
                                except:
                                    try:
                                        publish_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                                        break
                                    except:
                                        pass
                except:
                    pass
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
        if not publish_time:
            publish_time = datetime.now()
        
        return title, publish_time
    
    def smart_truncate_summary(self, text, target_length=300):
        """
        æ™ºèƒ½æˆªæ–­æ‘˜è¦ï¼šåœ¨300å­—å·¦å³æ‰¾åˆ°å¥å·æˆªæ–­
        """
        if not text:
            return ""
        
        # æ¸…ç†æ–‡æœ¬
        text = text.strip()
        
        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºç›®æ ‡é•¿åº¦ï¼Œç›´æ¥è¿”å›
        if len(text) <= target_length:
            return text
        
        # åœ¨ç›®æ ‡é•¿åº¦é™„è¿‘æŸ¥æ‰¾å¥å·
        search_start = max(0, target_length - 100)  # å‘å‰æœç´¢100å­—
        search_end = min(len(text), target_length + 100)  # å‘åæœç´¢100å­—
        
        # æŸ¥æ‰¾å¥å·ã€é—®å·ã€æ„Ÿå¹å·
        sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ']
        best_pos = target_length
        
        for pos in range(search_start, search_end):
            if text[pos] in sentence_endings:
                # æ£€æŸ¥åé¢æ˜¯å¦æœ‰ç©ºæ ¼æˆ–æ¢è¡Œ
                if pos + 1 < len(text) and text[pos + 1] in [' ', '\n', '\r', '\t']:
                    best_pos = pos + 1
                    break
        
        # å¦‚æœæ²¡æ‰¾åˆ°å¥å·ï¼Œåœ¨ç›®æ ‡é•¿åº¦å¤„æˆªæ–­
        summary = text[:best_pos].strip()
        
        # ç¡®ä¿æ‘˜è¦ä¸ä¸ºç©º
        if not summary:
            summary = text[:target_length].strip()
        
        return summary

    def init_database(self, db_path):
        """åˆå§‹åŒ–æ•°æ®åº“ï¼Œåˆ›å»ºè¡¨ç»“æ„ï¼ˆåªåœ¨è¡¨ä¸å­˜åœ¨æ—¶åˆ›å»ºï¼‰"""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute('''
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='history_news'
        ''')
        table_exists = cursor.fetchone() is not None
        
        if not table_exists:
            # åˆ›å»ºè¡¨ç»“æ„ï¼ˆä¸CSVç»“æ„ä¸€è‡´ï¼Œä½†å¢åŠ è‡ªå¢ä¸»é”®ï¼‰
            cursor.execute('''
                CREATE TABLE history_news (
                    index_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    id TEXT NOT NULL,
                    url TEXT UNIQUE NOT NULL,
                    title TEXT,
                    content TEXT,
                    summary TEXT,
                    source TEXT,
                    publish_time TEXT,
                    crawled_at TEXT
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢é€Ÿåº¦
            cursor.execute('CREATE INDEX idx_id ON history_news(id)')
            cursor.execute('CREATE INDEX idx_url ON history_news(url)')
            cursor.execute('CREATE INDEX idx_publish_time ON history_news(publish_time)')
            cursor.execute('CREATE INDEX idx_source ON history_news(source)')
            
            conn.commit()
            print(f"   âœ… åˆ›å»ºæ–°è¡¨: history_news")
        else:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ  index_id åˆ—ï¼ˆå…¼å®¹æ—§è¡¨ç»“æ„ï¼‰
            cursor.execute('PRAGMA table_info(history_news)')
            columns = [col[1] for col in cursor.fetchall()]
            
            if 'index_id' not in columns:
                # æ·»åŠ è‡ªå¢ä¸»é”®åˆ—
                print(f"   ğŸ”„ å‡çº§è¡¨ç»“æ„ï¼šæ·»åŠ  index_id åˆ—...")
                cursor.execute('''
                    CREATE TABLE history_news_new (
                        index_id INTEGER PRIMARY KEY AUTOINCREMENT,
                        id TEXT NOT NULL,
                        url TEXT UNIQUE NOT NULL,
                        title TEXT,
                        content TEXT,
                        summary TEXT,
                        source TEXT,
                        publish_time TEXT,
                        crawled_at TEXT
                    )
                ''')
                
                # è¿ç§»æ•°æ®
                cursor.execute('''
                    INSERT INTO history_news_new 
                    (id, url, title, content, summary, source, publish_time, crawled_at)
                    SELECT id, url, title, content, summary, source, publish_time, crawled_at
                    FROM history_news
                ''')
                
                # åˆ é™¤æ—§è¡¨ï¼Œé‡å‘½åæ–°è¡¨
                cursor.execute('DROP TABLE history_news')
                cursor.execute('ALTER TABLE history_news_new RENAME TO history_news')
                
                # é‡æ–°åˆ›å»ºç´¢å¼•
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_id ON history_news(id)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_url ON history_news(url)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_publish_time ON history_news(publish_time)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON history_news(source)')
                
                conn.commit()
                print(f"   âœ… è¡¨ç»“æ„å‡çº§å®Œæˆ")
            else:
                # ç¡®ä¿ç´¢å¼•å­˜åœ¨
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_id ON history_news(id)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_url ON history_news(url)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_publish_time ON history_news(publish_time)')
                cursor.execute('CREATE INDEX IF NOT EXISTS idx_source ON history_news(source)')
                conn.commit()
        
        conn.close()
    
    def load_checkpoint(self, db_path):
        """ä»æ•°æ®åº“åŠ è½½æ£€æŸ¥ç‚¹ï¼šè¿”å›å·²å¤„ç†çš„URLé›†åˆå’Œå·²ä¿å­˜çš„æ–‡ç« åˆ—è¡¨"""
        processed_urls = set()
        existing_articles = []
        
        if os.path.exists(db_path):
            try:
                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()
                
                # æŸ¥è¯¢æ‰€æœ‰å·²å¤„ç†çš„URL
                cursor.execute('SELECT url FROM history_news')
                urls = cursor.fetchall()
                processed_urls = set([url[0] for url in urls])
                
                # æŸ¥è¯¢æ‰€æœ‰æ–‡ç« ï¼ˆåŒ…å« index_idï¼‰
                cursor.execute('SELECT index_id, id, url, title, content, summary, source, publish_time, crawled_at FROM history_news')
                rows = cursor.fetchall()
                
                for row in rows:
                    existing_articles.append({
                        "index_id": row[0],
                        "id": row[1],
                        "url": row[2],
                        "title": row[3],
                        "content": row[4],
                        "summary": row[5],
                        "source": row[6],
                        "publish_time": row[7],
                        "crawled_at": row[8]
                    })
                
                conn.close()
                print(f"   ğŸ“‚ å‘ç°æ•°æ®åº“æ–‡ä»¶: {len(processed_urls)} æ¡å·²å¤„ç†")
            except Exception as e:
                print(f"   âš ï¸ è¯»å–æ•°æ®åº“å¤±è´¥: {e}")
        else:
            # å¦‚æœæ•°æ®åº“ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–å®ƒ
            self.init_database(db_path)
            print(f"   ğŸ“‚ åˆ›å»ºæ–°æ•°æ®åº“: {db_path}")
        
        return processed_urls, existing_articles
    
    def save_checkpoint(self, articles, db_path):
        """ä¿å­˜æ£€æŸ¥ç‚¹ï¼šå°†æ–‡ç« ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆindex_id è‡ªåŠ¨é€’å¢ï¼‰"""
        if not articles:
            return False
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # ç¡®ä¿è¡¨å­˜åœ¨
            self.init_database(db_path)
            
            # æ‰¹é‡æ’å…¥æˆ–æ›´æ–°æ–‡ç« ï¼ˆä½¿ç”¨ INSERT OR REPLACE é¿å…é‡å¤ï¼‰
            # index_id ä¼šè‡ªåŠ¨é€’å¢ï¼Œä¸éœ€è¦æ‰‹åŠ¨æŒ‡å®š
            for article in articles:
                cursor.execute('''
                    INSERT OR REPLACE INTO history_news 
                    (id, url, title, content, summary, source, publish_time, crawled_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    article.get('id', ''),
                    article.get('url', ''),
                    article.get('title', ''),
                    article.get('content', ''),
                    article.get('summary', ''),
                    article.get('source', ''),
                    str(article.get('publish_time', '')),
                    str(article.get('crawled_at', ''))
                ))
            
            conn.commit()
            conn.close()
            return True
        except Exception as e:
            print(f"   âš ï¸ ä¿å­˜æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def mine_history(self, site_name="CoinTelegraph", months_back=12, max_articles=None, db_path=None):
        """
        æŒ–æ˜å†å²æ–°é—»ï¼ˆæ”¯æŒä¸­æ–­æ¢å¤ï¼Œä½¿ç”¨SQLiteæ•°æ®åº“ï¼‰
        :param site_name: ç«™ç‚¹åç§°
        :param months_back: å›æº¯å¤šå°‘ä¸ªæœˆ
        :param max_articles: æœ€å¤§æ–‡ç« æ•°é‡ï¼ˆNone è¡¨ç¤ºä¸é™åˆ¶ï¼‰
        :param db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºä¸­æ–­æ¢å¤ï¼‰
        """
        if site_name not in self.sites:
            print(f"âŒ æœªçŸ¥ç«™ç‚¹: {site_name}")
            return pd.DataFrame()
        
        site_config = self.sites[site_name]
        main_sitemap = site_config["sitemap"]
        
        # è®¾ç½®æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆç»Ÿä¸€ä½¿ç”¨åŒä¸€ä¸ªæ•°æ®åº“æ–‡ä»¶ï¼‰
        if db_path is None:
            db_path = "history_news.db"  # ç»Ÿä¸€æ–‡ä»¶åï¼Œæ‰€æœ‰æ•°æ®å­˜åœ¨ä¸€ä¸ªè¡¨é‡Œ
        
        print(f"\nğŸ” å¼€å§‹æŒ–æ˜ {site_name} çš„å†å²æ•°æ®ï¼ˆè¿‡å» {months_back} ä¸ªæœˆï¼‰")
        print(f"ğŸ“… ç›®æ ‡å…³é”®è¯: {', '.join(self.target_keywords[:5])}...")
        if max_articles:
            print(f"ğŸ“Š æœ€å¤§æ–‡ç« æ•°é‡é™åˆ¶: {max_articles}")
        else:
            print(f"ğŸ“Š æ— æ•°é‡é™åˆ¶ï¼Œå°†çˆ¬å–æ‰€æœ‰åŒ¹é…çš„æ–°é—»")
        print(f"ğŸ’¾ æ•°æ®åº“æ–‡ä»¶: {db_path}")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database(db_path)
        
        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        processed_urls, existing_articles = self.load_checkpoint(db_path)
        articles = existing_articles.copy()
        
        if processed_urls:
            print(f"   âœ… ä»æ£€æŸ¥ç‚¹æ¢å¤: å·²å¤„ç† {len(processed_urls)} æ¡ï¼Œå°†ç»§ç»­å¤„ç†å‰©ä½™URL")
        
        # 1. è·å–æœˆåº¦ sitemap åˆ—è¡¨
        monthly_sitemaps = self.extract_monthly_sitemaps(main_sitemap)
        
        # é™åˆ¶åªå¤„ç†æœ€è¿‘ N ä¸ªæœˆçš„
        if len(monthly_sitemaps) > months_back:
            monthly_sitemaps = monthly_sitemaps[:months_back]
        
        # 2. ä»æ¯ä¸ªæœˆåº¦ sitemap æå–æ–‡ç« é“¾æ¥
        all_news_urls = []
        for sitemap_url in monthly_sitemaps:
            print(f"\n   ğŸ“‚ å¤„ç†: {sitemap_url}")
            urls = self.fetch_sitemap(sitemap_url)
            filtered = self.filter_news_urls(urls)
            all_news_urls.extend(filtered)
            print(f"   âœ… æå–åˆ° {len(filtered)} æ¡ç›¸å…³æ–°é—»é“¾æ¥")
            
            # å¦‚æœè®¾ç½®äº†é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦è¾¾åˆ°
            if max_articles and len(all_news_urls) >= max_articles:
                all_news_urls = all_news_urls[:max_articles]
                print(f"   âš ï¸ è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶ï¼Œåœæ­¢æ”¶é›†é“¾æ¥")
                break
        
        # è¿‡æ»¤æ‰å·²å¤„ç†çš„URL
        remaining_urls = [url for url in all_news_urls if url not in processed_urls]
        print(f"\nğŸ“Š å…±æ‰¾åˆ° {len(all_news_urls)} æ¡ç›¸å…³æ–°é—»é“¾æ¥")
        print(f"ğŸ“Š å·²å¤„ç† {len(processed_urls)} æ¡ï¼Œå‰©ä½™ {len(remaining_urls)} æ¡å¾…å¤„ç†")
        
        if not remaining_urls:
            print("âœ… æ‰€æœ‰URLå·²å¤„ç†å®Œæˆï¼")
            return pd.DataFrame(articles)
        
        # 3. çˆ¬å–æ–‡ç« å†…å®¹
        start_idx = len(processed_urls) + 1
        new_articles_count = 0  # è®°å½•æ–°å¢æ–‡ç« æ•°é‡
        
        for idx, url in enumerate(remaining_urls, start=start_idx):
            print(f"   [{idx}/{len(all_news_urls)}] çˆ¬å–: {url[:80]}...")
            
            try:
                content = self.extract_article_content(url)
                if content:
                    # æå–å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€å‘å¸ƒæ—¶é—´ï¼‰
                    title, publish_time = self.extract_article_metadata(url, content)
                    
                    # ç”Ÿæˆå”¯ä¸€ IDï¼ˆåŸºäº URL çš„ hashï¼‰
                    article_id = hashlib.md5(url.encode()).hexdigest()[:16]
                    
                    # æ™ºèƒ½æˆªæ–­æ‘˜è¦ï¼ˆ300å­—å·¦å³ï¼Œåœ¨å¥å·å¤„æˆªæ–­ï¼‰
                    summary = self.smart_truncate_summary(content, target_length=300)
                    
                    new_article = {
                        "id": article_id,
                        "url": url,
                        "title": title,
                        "content": content,  # å®Œæ•´å†…å®¹
                        "summary": summary,  # æ™ºèƒ½æˆªæ–­çš„æ‘˜è¦
                        "source": site_name,
                        "publish_time": str(publish_time) if publish_time else "",  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        "crawled_at": str(datetime.now())  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    }
                    
                    articles.append(new_article)
                    processed_urls.add(url)
                    new_articles_count += 1
                    
                    print(f"      âœ… æˆåŠŸæå–: {title[:50]}...")
                    
                    # æ¯5æ¡ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ï¼ˆåªä¿å­˜æ–°å¢çš„æ–‡ç« ï¼‰
                    if new_articles_count % 5 == 0:
                        # åªä¿å­˜æ–°å¢çš„æ–‡ç« ï¼Œé¿å…é‡å¤ä¿å­˜
                        new_articles = articles[len(existing_articles):]
                        if self.save_checkpoint(new_articles, db_path):
                            print(f"      ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆå…± {len(articles)} æ¡ï¼Œæ–°å¢ {new_articles_count} æ¡ï¼‰")
                else:
                    print(f"      âš ï¸ æ— æ³•æå–å†…å®¹")
            except KeyboardInterrupt:
                print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
                new_articles = articles[len(existing_articles):]
                if self.save_checkpoint(new_articles, db_path):
                    print(f"ğŸ’¾ å·²ä¿å­˜ {len(articles)} æ¡æ•°æ®åˆ° {db_path}")
                    print(f"ğŸ”„ ä¸‹æ¬¡è¿è¡Œå°†ä»ç¬¬ {len(articles) + 1} æ¡ç»§ç»­")
                raise
            except Exception as e:
                print(f"      âŒ æå–å¤±è´¥: {e}")
            
            # é¿å…è¯·æ±‚è¿‡å¿«ï¼ˆæ ¹æ®ç´¢å¼•è°ƒæ•´å»¶è¿Ÿï¼Œé¿å…è¢«å°ï¼‰
            # æ¯10æ¡è¯·æ±‚åå¢åŠ å»¶è¿Ÿ
            if idx % 10 == 0:
                time.sleep(2)  # æ¯10æ¡ä¼‘æ¯2ç§’
            elif idx % 5 == 0:
                time.sleep(1)  # æ¯5æ¡ä¼‘æ¯1ç§’
            else:
                time.sleep(0.8)  # åŸºç¡€å»¶è¿Ÿ0.8ç§’
        
        # æœ€ç»ˆä¿å­˜ï¼ˆåªä¿å­˜æ–°å¢çš„æ–‡ç« ï¼‰
        new_articles = articles[len(existing_articles):]
        if new_articles and self.save_checkpoint(new_articles, db_path):
            print(f"\nğŸ’¾ æœ€ç»ˆä¿å­˜: {len(articles)} æ¡æ•°æ®ï¼ˆæ–°å¢ {len(new_articles)} æ¡ï¼‰")
        
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")
        
        # ä»æ•°æ®åº“è¯»å–æ‰€æœ‰æ•°æ®è¿”å›DataFrameï¼ˆåŒ…å« index_idï¼‰
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query('SELECT * FROM history_news ORDER BY publish_time DESC', conn)
        conn.close()
        
        return df

    def mine_recent_sitemap(self, site_name="CoinTelegraph", days_back=30):
        """
        å¿«é€ŸæŒ–æ˜æœ€è¿‘ N å¤©çš„æ–°é—»ï¼ˆç”¨äºæ¯æ—¥æ›´æ–°ï¼‰
        """
        if site_name not in self.sites:
            print(f"âŒ æœªçŸ¥ç«™ç‚¹: {site_name}")
            return pd.DataFrame()
        
        site_config = self.sites[site_name]
        main_sitemap = site_config["sitemap"]
        
        print(f"\nğŸ” å¿«é€ŸæŒ–æ˜ {site_name} æœ€è¿‘ {days_back} å¤©çš„æ–°é—»")
        
        # è·å–æœ€è¿‘çš„æœˆåº¦ sitemap
        monthly_sitemaps = self.extract_monthly_sitemaps(main_sitemap)
        recent_sitemaps = monthly_sitemaps[:2]  # æœ€è¿‘2ä¸ªæœˆ
        
        all_news_urls = []
        for sitemap_url in recent_sitemaps:
            urls = self.fetch_sitemap(sitemap_url)
            filtered = self.filter_news_urls(urls)
            all_news_urls.extend(filtered)
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(all_news_urls)} æ¡ç›¸å…³æ–°é—»é“¾æ¥")
        
        # åªçˆ¬å–å‰50æ¡ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
        articles = []
        
        for idx, url in enumerate(all_news_urls[:50], 1):
            print(f"   [{idx}/50] çˆ¬å–: {url[:60]}...")
            try:
                content = self.extract_article_content(url)
                if content:
                    title, publish_time = self.extract_article_metadata(url, content)
                    article_id = hashlib.md5(url.encode()).hexdigest()[:16]
                    summary = self.smart_truncate_summary(content, target_length=300)
                    
                    articles.append({
                        "id": article_id,
                        "url": url,
                        "title": title,
                        "content": content,
                        "summary": summary,
                        "source": site_name,
                        "publish_time": publish_time,
                        "crawled_at": datetime.now()
                    })
            except Exception as e:
                print(f"      âš ï¸ æå–å¤±è´¥: {e}")
            
            time.sleep(0.8)  # åŸºç¡€å»¶è¿Ÿ
        
        return pd.DataFrame(articles)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å†å²æ–°é—»æŒ–æ˜å·¥å…·')
    parser.add_argument('--mode', choices=['full', 'recent'], default='recent',
                       help='æŒ–æ˜æ¨¡å¼: full=å®Œæ•´å†å², recent=æœ€è¿‘30å¤©')
    parser.add_argument('--months', type=int, default=12,
                       help='å›æº¯æœˆæ•°ï¼ˆä»…ç”¨äº full æ¨¡å¼ï¼‰')
    parser.add_argument('--max', type=int, default=None,
                       help='æœ€å¤§æ–‡ç« æ•°é‡ï¼ˆé»˜è®¤ä¸é™åˆ¶ï¼‰')
    
    args = parser.parse_args()
    
    miner = HistoryNewsMiner()
    
    if args.mode == 'full':
        print("ğŸš€ å†·å¯åŠ¨æ¨¡å¼ï¼šæŒ–æ˜å®Œæ•´å†å²æ•°æ®")
        # ç»Ÿä¸€ä½¿ç”¨åŒä¸€ä¸ªæ•°æ®åº“æ–‡ä»¶ï¼Œæ‰€æœ‰æ•°æ®å­˜åœ¨ä¸€ä¸ªè¡¨é‡Œ
        db_path = "history_news.db"
        df = miner.mine_history(
            site_name="CoinTelegraph",
            months_back=args.months,
            max_articles=args.max,  # None è¡¨ç¤ºä¸é™åˆ¶
            db_path=db_path
        )
    else:
        print("ğŸš€ å¿«é€Ÿæ¨¡å¼ï¼šæŒ–æ˜æœ€è¿‘30å¤©æ•°æ®")
        df = miner.mine_recent_sitemap(
            site_name="CoinTelegraph",
            days_back=30
        )
    
    if not df.empty:
        # æ•°æ®å·²ç»åœ¨æ•°æ®åº“ä¸­ä¿å­˜äº†ï¼Œè¿™é‡Œåªæ˜¯ç¡®è®¤
        print(f"\nâœ… æ•°æ®æŒ–æ˜å®Œæˆï¼")
        print(f"ğŸ“Š å…± {len(df)} æ¡å†å²æ–°é—»")
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åœ¨æ•°æ®åº“ä¸­")
        
        # å¯é€‰ï¼šå¯¼å‡ºä¸ºCSVå¤‡ä»½
        csv_backup = f"history_news_backup_{datetime.now().strftime('%Y%m%d')}.csv"
        df.to_csv(csv_backup, index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ CSVå¤‡ä»½å·²ä¿å­˜åˆ°: {csv_backup}")
    else:
        print("âš ï¸ æœªè·å–åˆ°æ•°æ®")

